{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import re\n",
    "import os\n",
    "import nltk\n",
    "import requests\n",
    "import unidecode\n",
    "import unicodedata\n",
    "import contractions\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from googletrans import Translator\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loadinng credenttials as environmen variables\n",
    "load_dotenv('data/twitter_kafka_credentials.env', override = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get city coordinates\n",
    "geolocator = Nominatim(user_agent = 'bmartin')\n",
    "\n",
    "# Get current date\n",
    "today = dt.date.today()\n",
    "today = today.strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting twitter credentials\n",
    "twitter_key = os.environ.get('api_key')\n",
    "twitter_secret_key = os.environ.get('secret_key')\n",
    "bearer_token = os.environ.get('bearer_token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting twitter credentials\n",
    "twitter_key1 = os.environ.get('api_key1')\n",
    "twitter_secret_key1 = os.environ.get('secret_key1')\n",
    "bearer_token1 = os.environ.get('bearer_token1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_tweets(query, bearer_token = bearer_token):    \n",
    "    \n",
    "    headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
    "\n",
    "    url = f\"https://api.twitter.com/2/tweets/search/recent?query={query}&\"\n",
    "\n",
    "    params = {\n",
    "        # select specific Tweet fields from each returned Tweet object\n",
    "        'tweet.fields': 'text,created_at,lang,possibly_sensitive', # public_metrics\n",
    "        \n",
    "        # maximum number of search results to be returned (10 - 100)\n",
    "        'max_results': 100,\n",
    "        \n",
    "        'expansions': 'author_id,referenced_tweets.id,geo.place_id',\n",
    "        \n",
    "        \"place.fields\": 'country,full_name,name',\n",
    "        \n",
    "        \"user.fields\": 'location'\n",
    "        \n",
    "        #\"next_token\": \"b26v89c19zqg8o3fpdy6tr1fnbfjofv5hd12kcjsua4xp\"\n",
    "    }\n",
    "    \n",
    "    # request\n",
    "    response = requests.get(url = url, params = params, headers = headers)\n",
    "\n",
    "    # verify successfull request\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "        \n",
    "    else:\n",
    "        return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'includes', 'meta'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# search term\n",
    "search_tweet = search_tweets(query = \"Black Widow\")\n",
    "\n",
    "# 4 main keys\n",
    "search_tweet.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'newest_id': '1464772543886278657',\n",
       " 'oldest_id': '1464760672412221449',\n",
       " 'result_count': 100,\n",
       " 'next_token': 'b26v89c19zqg8o3fpdy797c74n18b39pyget7rzrzz33x'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_tweet['meta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['users', 'tweets'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_tweet['includes'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframes(json_tweets, today):\n",
    "\n",
    "    if \"places\" in search_tweet['includes'].keys():\n",
    "        \n",
    "        # If the field exists, create a dataframe with the corresponding data\n",
    "        places = pd.json_normalize(search_tweet['includes']['places']).rename(columns = {\"id\":\"geo.place_id\"})\n",
    "        \n",
    "        # Create users dataframe\n",
    "        users = pd.json_normalize(search_tweet['includes']['users']).rename(columns = {\"id\":\"user_id\"})\n",
    "    \n",
    "        # Create df with tweet's data\n",
    "        tweets = pd.json_normalize(search_tweet['data']).rename(columns = {\"id\":\"tweet_id\"})\n",
    "        \n",
    "        # Get tweet's type\n",
    "        tweets['type'] = tweets.referenced_tweets.apply(lambda x: x[0][\"type\"] if type(x) == list else None)\n",
    "        \n",
    "        # Drop retweeted tweets\n",
    "        tweets = tweets[tweets[\"type\"] != \"retweeted\"].reset_index(drop = True)\n",
    "        \n",
    "        # List of users in tweets dataframe\n",
    "        user_list = tweets.author_id.unique()\n",
    "\n",
    "        # Only keep users from tweets dataframe\n",
    "        users = users.loc[users.user_id.isin(user_list)].reset_index(drop = True)\n",
    "        \n",
    "        # Drop cols\n",
    "        tweets = tweets.drop(['referenced_tweets','author_id','geo.place_id'], axis = 1)\n",
    "        \n",
    "        # Save data\n",
    "        #tweets.to_csv(f\"data/tweets_{today}.csv\", index = False)\n",
    "        #users.to_csv(f\"users/users_{today}.csv\", index = False)\n",
    "        #places.to_csv(f\"places/places_{today}.csv\", index = False)\n",
    "        \n",
    "        return tweets, users, places\n",
    "        \n",
    "    else: \n",
    "        # Create users dataframe\n",
    "        users = pd.json_normalize(search_tweet['includes']['users']).rename(columns = {\"id\":\"user_id\"})\n",
    "    \n",
    "        # Create df with tweet's data\n",
    "        tweets = pd.json_normalize(search_tweet['data']).rename(columns = {\"id\":\"tweet_id\"})\n",
    "        \n",
    "        # Get tweet's type\n",
    "        tweets['type'] = tweets.referenced_tweets.apply(lambda x: x[0][\"type\"] if type(x) == list else None)\n",
    "        \n",
    "        # Drop retweeted tweets\n",
    "        tweets = tweets[tweets[\"type\"] != \"retweeted\"].reset_index(drop = True)\n",
    "        \n",
    "        # List of users in tweets dataframe\n",
    "        user_list = tweets.author_id.unique()\n",
    "\n",
    "        # Only keep users from tweets dataframe\n",
    "        users = users.loc[users.user_id.isin(user_list)].reset_index(drop = True)\n",
    "        \n",
    "        # Drop cols\n",
    "        tweets = tweets.drop(['referenced_tweets','author_id'], axis = 1)\n",
    "        \n",
    "        # Save data\n",
    "        #tweets.to_csv(f\"data/tweets_{today}.csv\", index = False)\n",
    "        #users.to_csv(f\"users/users_{today}.csv\", index = False)\n",
    "        \n",
    "        return tweets, users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"places\" in search_tweet['includes'].keys():\n",
    "    tweets, users, places = create_dataframes(search_tweet, today)\n",
    "    \n",
    "else:\n",
    "    tweets, users = create_dataframes(search_tweet, today)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessor:\n",
    "    \n",
    "    def __init__(self, regex_dict = None):\n",
    "        \n",
    "        # creating classes\n",
    "        # stem\n",
    "        self.sb = nltk.stem.SnowballStemmer('english')\n",
    "        \n",
    "        # lemmatize\n",
    "        self.lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "        \n",
    "        # translate\n",
    "        self.translator = Translator()\n",
    "        \n",
    "        # declare a default regex dict\n",
    "        self.default_regex_dict = {'goo[o]*d':'good', '2morrow':'tomorrow', 'b4':'before', 'otw':'on the way',\n",
    "                                   'idk':\"i don't know\", ':)':'smile', 'bc':'because', '2nite':'tonight',\n",
    "                                   'yeah':'yes', 'yeshhhhhhhh':'yes', ' yeeeee':'yes', 'btw':'by the way', \n",
    "                                   'fyi':'for your information', 'gr8':'great', 'asap':'as soon as possible', \n",
    "                                   'yummmmmy':'yummy', 'gf':'girlfriend', 'thx':'thanks','nowwwwwww':'now', \n",
    "                                   ' ppl ':' people ', 'yeiii':'yes'}\n",
    "        \n",
    "        # if no regex_dict defined by user, then use \n",
    "        # one by default. Else, concat two regex dicts\n",
    "        if regex_dict:            \n",
    "            self.regex_dict = {**regex_dict, **default_regex_dict}\n",
    "            \n",
    "        else:\n",
    "            self.regex_dict = self.default_regex_dict\n",
    "    \n",
    "    def translate_twt(self, pdf):\n",
    "    \n",
    "        \"\"\"\n",
    "        This function helps to translate a tweet from any \n",
    "        language to English.\n",
    "\n",
    "        Inputs:\n",
    "            - pdf: Pandas dataframe. This dataframe must have\n",
    "               the following columns:\n",
    "                - lang: Tweet's language.\n",
    "                - clean_tweet: Partially pre-processed tweet.\n",
    "\n",
    "        Outputs: Translated tweet from any language available \n",
    "                 in googletrans api to English.\n",
    "        \"\"\"\n",
    "\n",
    "        # Check if the language of the tweet is either undefined or English\n",
    "        # to avoid translation.\n",
    "        if pdf[\"lang\"] == \"und\" or pdf[\"lang\"] == \"en\":\n",
    "            pdf[\"translated_tweet\"] = pdf[\"clean_tweet\"]\n",
    "\n",
    "        # Check if tweet is in Hindi. The code of Hindi language is \"hi\", but \n",
    "        # Twitter has defined the code as \"in\".\n",
    "        elif pdf[\"lang\"] == \"in\":\n",
    "            pdf[\"translated_tweet\"] = self.translator.translate(pdf[\"clean_tweet\"], src = \"hi\", dest = \"en\").text\n",
    "\n",
    "        # For any other language the translator should work just fine, so the\n",
    "        # api should work with the language detected by Twitter.\n",
    "        else:\n",
    "            pdf[\"translated_tweet\"] = self.translator.translate(pdf[\"clean_tweet\"], src = pdf[\"lang\"], dest = \"en\").text\n",
    "\n",
    "        return pdf[\"translated_tweet\"]\n",
    "\n",
    "    \n",
    "    def removeNoise(self, pdf):\n",
    "        \n",
    "        \"\"\"\n",
    "        Function to remove noise from strings. \n",
    "        \n",
    "        Inputs: A pandas dataframe with raw strings of length n.\n",
    "        \n",
    "        Output: A clean string where elements such as accented \n",
    "        words, html tags, punctuation marks, and extra white \n",
    "        spaces will be removed (or transform) if it's the case.\n",
    "        \"\"\"\n",
    "        \n",
    "        # to lower case\n",
    "        pdf[\"clean_tweet\"] = pdf.text.apply(lambda x: x.lower())\n",
    "        \n",
    "        # remove accented characters from string\n",
    "        # e.g. canciÃ³n --> cancion\n",
    "        pdf[\"clean_tweet\"] = pdf.clean_tweet.apply(lambda x: unidecode.unidecode(x))\n",
    "        \n",
    "        # remove html tags \n",
    "        pdf[\"clean_tweet\"] = pdf.clean_tweet.str.replace(r'<[^<>]*>', '', regex=True)\n",
    "        \n",
    "        # remove (match with) usernames | hashtags | punct marks | links\n",
    "        # punct marks = \",.':!?;\n",
    "        # do not remove: ' \n",
    "        # but remove: \"\n",
    "        pdf[\"clean_tweet\"] = pdf.clean_tweet.apply(lambda x:' '.join(re.sub(\"(@[A-Za-z0-9]+)|(#[A-Za-z0-9]+)|([-.,:_;])|(https?:\\/\\/.*[\\r\\n]*)\",\n",
    "                                                                            \" \", x).split()).replace('\"',''))\n",
    "                \n",
    "        # remove white spaces at the begining and at \n",
    "        # the end of a string\n",
    "        pdf['clean_tweet'] = pdf.clean_tweet.apply(lambda x: x.lstrip(' '))\n",
    "        pdf['clean_tweet'] = pdf.clean_tweet.apply(lambda x: x.rstrip(' '))\n",
    "        \n",
    "        # Translate tweet\n",
    "        pdf[\"clean_tweet\"] = pdf.apply(lambda x: self.translate_twt(x), axis = 1)\n",
    "        \n",
    "        # normalize string\n",
    "        # normalize accented charcaters and other strange characters\n",
    "        # NFKD if there are accented characters (????\n",
    "        pdf[\"clean_tweet\"] = pdf.clean_tweet.apply(lambda x: unicodedata.normalize('NFKC', x).encode('ASCII', 'ignore').decode(\"utf-8\"))\n",
    "        \n",
    "        return pdf\n",
    "    \n",
    "    def apply_regex(self, pdf):\n",
    "    \n",
    "        # Loop over dataframe index (records)\n",
    "        for idx in pdf.index:\n",
    "            \n",
    "            # Loop over regex_dict (keys and values)\n",
    "            for k,v in zip(self.regex_dict.keys(), self.regex_dict.values()):\n",
    "                \n",
    "                # Replace string if needed\n",
    "                pdf[\"clean_tweet\"].iloc[idx] = pdf[\"clean_tweet\"].iloc[idx].replace(k, v)\n",
    "          \n",
    "        # Return col\n",
    "        return pdf[\"clean_tweet\"]\n",
    "    \n",
    "    \n",
    "    def textNormalization(self, pdf):\n",
    "        \n",
    "        \"\"\"\n",
    "        Function to normalize a string. \n",
    "        \n",
    "        Inputs: A pandas dataframe with strings (of length n) that \n",
    "        will be normalized. \n",
    "        \n",
    "        Outputs: A normalized string whitout noise, words in their\n",
    "        (expected) correct form and with no stopwords.\n",
    "        \"\"\"\n",
    "        \n",
    "        # remove noise first\n",
    "        pdf = self.removeNoise(pdf)\n",
    "\n",
    "        # expand contractions\n",
    "        # e.g. don't --> do not\n",
    "        pdf['clean_tweet'] = pdf.clean_tweet.apply(lambda x: contractions.fix(x))\n",
    " \n",
    "        # Normalize words\n",
    "        #pdf['clean_tweet'] = self.apply_regex(pdf)\n",
    "        pdf['clean_tweet'] = pdf.clean_tweet.replace(self.regex_dict)\n",
    "                \n",
    "        # get English stopwords    \n",
    "        stop_words = stopwords.words('english')\n",
    "        stopwords_dict = Counter(stop_words)\n",
    "        \n",
    "        # remove stopwords from string\n",
    "        pdf[\"clean_tweet\"] = pdf.clean_tweet.apply(lambda x: ' '.join([word for word in x.split()\n",
    "                                                                       if word not in stopwords_dict]))\n",
    "            \n",
    "        return pdf\n",
    "    \n",
    "    \n",
    "    def wordTokenize(self, pdf):\n",
    "        \"\"\"\n",
    "        Function to tokenize a string into words. Tokenization is a way \n",
    "        of separating a piece of text into smaller units called tokens.\n",
    "        In this case tokens are words (but can also be characters or \n",
    "        subwords).\n",
    "        \n",
    "        Inputs: A pandas dataframe with strings (of length n) that will be tokenized. \n",
    "        \n",
    "        Outputs: A list of tokenized words.\n",
    "        \"\"\"\n",
    "        # string normalized\n",
    "        #normalized = self.textNormalization(string)\n",
    "        pdf = self.textNormalization(pdf)\n",
    "        \n",
    "        # Use word_tokenize method to split the string\n",
    "        # into individual words. By default it returns\n",
    "        # a list.\n",
    "        pdf[\"clean_tweet\"] = pdf.clean_tweet.apply(lambda x: nltk.word_tokenize(x))        \n",
    "        \n",
    "        # Using isalpha() will help us to only keep\n",
    "        # items from the alphabet (no punctuation\n",
    "        # marks). \n",
    "        #pdf[\"clean_tweet\"] = pdf.clean_tweet.apply(lambda x: [word for word in x if word.isalpha()])\n",
    "        \n",
    "        # Keep only unique elements\n",
    "        pdf[\"clean_tweet\"] = pdf.clean_tweet.apply(lambda x: list(set(x)))\n",
    "\n",
    "        # return list of tokenized words by row\n",
    "        return pdf\n",
    "    \n",
    "    \n",
    "    def phraseTokenize(self, pdf):\n",
    "        \n",
    "        \"\"\"\n",
    "        Function to tokenize a string into sentences. Tokenization is\n",
    "        a way of separating a piece of text into smaller units called\n",
    "        tokens. In this case tokens are phrases (but can also be words,\n",
    "        characters or subwords).\n",
    "        \n",
    "        Inputs: A string (of length n) that will be tokenized. \n",
    "        \n",
    "        Outputs: A list of tokenized sentences.\n",
    "        \"\"\"\n",
    "        \n",
    "        # pandas dataframe with strings normalized\n",
    "        pdf = self.textNormalization(pdf)\n",
    "        \n",
    "        # Use sent_tokenize method to split the string\n",
    "        # into sentences. By default it returns a list.\n",
    "        pdf[\"clean_tweet\"] = pdf.clean_tweet.apply(lambda x: nltk.sent_tokenize(x))   \n",
    "        \n",
    "        return pdf \n",
    "    \n",
    "    \n",
    "    def stemWords(self, pdf):\n",
    "        \n",
    "        \"\"\"\n",
    "        Function to stem strings. Stemming is the process of reducing\n",
    "        a word to its word stem that affixes to suffixes and prefixes \n",
    "        or to the roots of words (known as a lemma).\n",
    "        \n",
    "        Inputs: A raw string of length n.\n",
    "        \n",
    "        Output: Roots of each word of a given string.\n",
    "        \"\"\"\n",
    "        \n",
    "        # pandas dataframe with strings normalized\n",
    "        pdf = self.textNormalization(pdf)\n",
    "        \n",
    "        # tokenized string (into words)\n",
    "        pdf = self.wordTokenize(data)\n",
    "            \n",
    "        # reduct words to its root    \n",
    "        pdf[\"clean_tweet\"] = pdf.clean_tweet.apply(lambda x: [self.sb.stem(word) for word in x])\n",
    "        \n",
    "        return pdf\n",
    "    \n",
    "    \n",
    "    def lemmatizeWords(self, pdf):\n",
    "        \n",
    "        \"\"\"\n",
    "        Function to lemmatize strings. Lemmatization is a method \n",
    "        responsible for grouping different inflected forms of \n",
    "        words into the root form, having the same meaning. It is \n",
    "        similar to stemming.\n",
    "        \n",
    "        Inputs: A raw string of length n.\n",
    "        \n",
    "        Output: Roots of each word of a given string (with better\n",
    "        performance than in stemming).\n",
    "        \"\"\"\n",
    "        \n",
    "        # pandas dataframe with strings normalized\n",
    "        pdf = self.textNormalization(pdf)\n",
    "        \n",
    "        # list of tokenized words (from string)\n",
    "        # Here it was decided to tokenize by words\n",
    "        # rather than by sentences due to we thought\n",
    "        # it would be easier to find the correct roots\n",
    "        # of each word.\n",
    "        pdf = self.wordTokenize(pdf)\n",
    "        \n",
    "        # lematize word from list of tokenized words\n",
    "        #lematized = [self.lemmatizer.lemmatize(word) for word in tokenized]\n",
    "        pdf[\"clean_tweet\"] = pdf.clean_tweet.apply(lambda x: [self.lemmatizer.lemmatize(word) for word in x])\n",
    "        \n",
    "        return pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create class object\n",
    "pre_processor = PreProcessor()\n",
    "\n",
    "# Clean data and only keep \n",
    "# the roots of each word.\n",
    "#tweets['clean_tweet'] = tweets.text.apply(pre_processor.removeNoise)\n",
    "tweets = pre_processor.lemmatizeWords(tweets)\n",
    "tweets.head(2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tweets.to_csv(f'clean_data/tweets_{today}.csv', index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
