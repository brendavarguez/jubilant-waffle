{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import re\n",
    "import os\n",
    "import nltk\n",
    "import requests\n",
    "import unidecode\n",
    "import unicodedata\n",
    "import contractions\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from googletrans import Translator\n",
    "#from geopy.geocoders import Nominatim\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loadinng credenttials as environmen variables\n",
    "load_dotenv('twitter_kafka_credentials.env', override = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2021-11-28'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to get city coordinates\n",
    "#geolocator = Nominatim(user_agent = 'bmartin')\n",
    "\n",
    "# Get current date\n",
    "today = dt.date.today()\n",
    "today = today.strftime(\"%Y-%m-%d\")\n",
    "today"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Twitter API\n",
    "\n",
    "### 1.1 About the Twitter API\n",
    "\n",
    "The Twitter API can be used to retrieve and analyze data, as well as engage with the conversation on Twitter. It provides access to a variety of different resources including:\n",
    "- Tweets\n",
    "- Users\n",
    "- Direct Messages\n",
    "- Lists\n",
    "- Trends\n",
    "- Media\n",
    "- Places\n",
    "\n",
    "The Twitter API currently consists of two supported versions, as well as different access tiers. \n",
    "- **Standard v1.1**: The legacy standard endpoints provide access to the following resources with the standard v1.1 offerings.\n",
    "    \n",
    "    - Get Tweet timelines\n",
    "    - Curate a collection of Tweets\n",
    "    - Filter realtime Tweets\n",
    "    - Sample realtime Tweets\n",
    "    - Manage and pull public account information\n",
    "    - Get trends near a location\n",
    "    - Get locations with trending topics\n",
    "    - Get information about a place    \n",
    "    \n",
    "    \n",
    "- **Twitter API v2 Early Access**: A new Twitter API is being build with a modern and more sustainable foundation as well as an improved developer experience. The first endpoints are now available within Early Access, and enable users to listen to and analyze the public conversation. Additional endpoints, features, and access levels will be released soon.\n",
    "    \n",
    "    - Ability to request specific objects and fields.\n",
    "    - New and more detailed data objects\n",
    "    - Advanced metrics return in Tweets (including impressions, video views, user profile and URL clicks)\n",
    "    - Insights on Tweet topics with annotations (filter by topic using `entity` and `context` operators)\n",
    "    - Improved conversation tracking\n",
    "    - Academic Research product track (grants free access to full-archive search)\n",
    "    - High confidence spam filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting twitter credentials\n",
    "twitter_key = os.environ.get('api_key')\n",
    "twitter_secret_key = os.environ.get('secret_key')\n",
    "bearer_token = os.environ.get('bearer_token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting twitter credentials\n",
    "twitter_key1 = os.environ.get('api_key1')\n",
    "twitter_secret_key1 = os.environ.get('secret_key1')\n",
    "bearer_token1 = os.environ.get('bearer_token1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Retrieve data\n",
    "### 1.2.1 Search Tweets\n",
    "\n",
    "The [search endpoint](https://developer.twitter.com/en/docs/twitter-api/tweets/search/api-reference/get-tweets-search-recent) returns Tweets from the last seven days that match a search query. Parameters are listed below:\n",
    "- `query`(required): rule for matching Tweets.\n",
    "- `expansions`: Expansions enable users to request additional data objects that relate to the originally returned Tweets.\n",
    "- `max_results`: The maximum number of results to be returned.\n",
    "- `next_token`: This parameter is used to get the next 'page' of results. \n",
    "- `place.fields`: Enables to select specific place fields that will be delivered in each returned Tweet. \n",
    "- `tweet.fields`: Enables to select specific Tweet fields that will be delivered in each returned Tweet object.\n",
    "- `user.fields`: Enables to select specific user fields that will be delivered in each returned Tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_tweets(query, bearer_token = bearer_token, next_token = None):    \n",
    "    \n",
    "    \"\"\"\n",
    "    Function to request tweets according to a specific query.\n",
    "    \n",
    "    Inputs:\n",
    "        - query: A string that will be used to find tweets.\n",
    "                 Tweets must match this string to be returned.\n",
    "        - bearer_token: Security token from Twitter API.\n",
    "        - next_token: ID of the next page that matches the specified query.\n",
    "        \n",
    "    Outputs: Dictionary (json type) with the requested data.  \n",
    "    \"\"\"\n",
    "    \n",
    "    headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
    "    \n",
    "    # end point\n",
    "    url = f\"https://api.twitter.com/2/tweets/search/recent?query={query}&\"\n",
    "\n",
    "    params = {\n",
    "        # select specific Tweet fields from each returned Tweet object\n",
    "        'tweet.fields': 'text,created_at,lang,possibly_sensitive', # public_metrics\n",
    "        \n",
    "        # maximum number of search results to be returned (10 - 100)\n",
    "        'max_results': 100,\n",
    "        \n",
    "        # additional data that relate to the originally returned Tweets\n",
    "        'expansions': 'author_id,referenced_tweets.id,geo.place_id',\n",
    "        \n",
    "        # select specific place fields \n",
    "        \"place.fields\": 'country,full_name,name',\n",
    "        \n",
    "        # select specific user fields\n",
    "        \"user.fields\": 'location',\n",
    "        \n",
    "        # get the next page of results.\n",
    "        \"next_token\": next_token\n",
    "    }\n",
    "    \n",
    "    # request\n",
    "    response = requests.get(url = url, params = params, headers = headers)\n",
    "\n",
    "    # verify successfull request\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "        \n",
    "    else:\n",
    "        return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'includes', 'meta'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# search term\n",
    "search_tweet = search_tweets(query = \"Black Widow\")\n",
    "\n",
    "# 4 main keys\n",
    "search_tweet.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate some data\n",
    "\n",
    "After having requested tweets data, some dataframes were generated with different data.\n",
    "\n",
    "- **tweets**: Pandas dataframe with information about tweets. The data from this dataframe is the required to perform text classification as well as to generate some visualizations. Columns:\n",
    "    - `text`: Tweet content.\n",
    "    - `lang`: Tweets' original language. Although some tweets are not in English, during the pre-processing process tweets are translated from the origin language to English.\n",
    "    - `possibly_sensitive`: Boolean. Specifies if the tweet might be sensitive. \n",
    "    - `tweet_id`: Tweet's unique identifier.\n",
    "    - `created_at`: When the tweet was created (tweeted).\n",
    "    - `type`: Type of tweet: original tweet, replied (reply from another tweet), quoted or retweeted.\n",
    "    \n",
    "    \n",
    "- **users**: Pandas dataframe with users information. Columns:\n",
    "    - `user_id`: User's unique identifier.\n",
    "    - `username`: User's username.\n",
    "    - `name`: Name that is displayed on Twitter.\n",
    "    - `location`: User's location. In Twitter there is this field in the user's biography were users can specify their location. Maybe this location is where they were born, where they currently live or just a random place. However, many users do not really include a geographical location in there, some of them just write something else such as their pronouns. So this field do not necessary specifies a geographical location.\n",
    "\n",
    "\n",
    "- **places**: Pandas dataframe about places where a tweet was tweeted.\n",
    "    - `country`: Country where a tweet was tweeted.\n",
    "    - `full_name`: City, countrty where a tweet was tweeted.\n",
    "    - `geo.place_id`: Unique identifier of location.\n",
    "    - `name`: City name where a tweet was tweeted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframes(json_tweets, today):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to create and organize different data into specific data frames.\n",
    "    \n",
    "    Inputs:\n",
    "        - json_tweets: A dictionary with tweets data.\n",
    "    \n",
    "    Outputs: \n",
    "        - tweets: Pandas dataframe with relevant information about tweets (to\n",
    "                  further perform text classification).\n",
    "                  \n",
    "        - users: Pandas dataframe with users information.\n",
    "        \n",
    "        - places (optional): Pandas dataframe about places where users tweeted. If not a \n",
    "                  single tweets contains the place where it was tweeted, then\n",
    "                  this dataframe will not be returned.\n",
    "    \"\"\"\n",
    "\n",
    "    # Not all users enable their location when tweeting, so\n",
    "    # we need to check if there are available locations for\n",
    "    # the tweets returned.\n",
    "    if \"places\" in json_tweets['includes'].keys():\n",
    "        \n",
    "        # If the field exists, create a dataframe with the corresponding data\n",
    "        places = pd.json_normalize(json_tweets['includes']['places']).rename(columns = {\"id\":\"geo.place_id\"})\n",
    "        \n",
    "        # Create users dataframe\n",
    "        users = pd.json_normalize(json_tweets['includes']['users']).rename(columns = {\"id\":\"user_id\"})\n",
    "    \n",
    "        # Create df with tweet's data\n",
    "        tweets = pd.json_normalize(json_tweets['data']).rename(columns = {\"id\":\"tweet_id\"})\n",
    "        \n",
    "        # Get tweet's type\n",
    "        tweets['type'] = tweets.referenced_tweets.apply(lambda x: x[0][\"type\"] if type(x) == list else None)\n",
    "        \n",
    "        # Drop retweeted tweets\n",
    "        tweets = tweets[tweets[\"type\"] != \"retweeted\"].reset_index(drop = True)\n",
    "        \n",
    "        # List of users in tweets dataframe to only \n",
    "        # keep users from tweets dataframe\n",
    "        user_list = tweets.author_id.unique()\n",
    "        users = users.loc[users.user_id.isin(user_list)].reset_index(drop = True)\n",
    "        \n",
    "        # Drop cols\n",
    "        tweets = tweets.drop(['referenced_tweets','author_id','geo.place_id'], axis = 1)\n",
    "        return tweets, users, places\n",
    "    \n",
    "    # Only return users and tweets dataframes since any tweet \n",
    "    # contained information about the place where it was tweeted.\n",
    "    else: \n",
    "        # Create users dataframe\n",
    "        users = pd.json_normalize(json_tweets['includes']['users']).rename(columns = {\"id\":\"user_id\"})\n",
    "    \n",
    "        # Create df with tweet's data\n",
    "        tweets = pd.json_normalize(json_tweets['data']).rename(columns = {\"id\":\"tweet_id\"})\n",
    "        \n",
    "        # Get tweet's type\n",
    "        tweets['type'] = tweets.referenced_tweets.apply(lambda x: x[0][\"type\"] if type(x) == list else None)\n",
    "        \n",
    "        # Drop retweeted tweets\n",
    "        #tweets = tweets[tweets[\"type\"] != \"retweeted\"].reset_index(drop = True)\n",
    "        \n",
    "        # List of users in tweets dataframe\n",
    "        user_list = tweets.author_id.unique()\n",
    "\n",
    "        # Only keep users from tweets dataframe\n",
    "        users = users.loc[users.user_id.isin(user_list)].reset_index(drop = True)\n",
    "        \n",
    "        # Drop cols\n",
    "        tweets = tweets.drop(['referenced_tweets','author_id'], axis = 1)\n",
    "        return tweets, users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we have tweet's location\n",
    "if \"places\" in search_tweet['includes'].keys():\n",
    "    main_tweets, main_users, main_places = create_dataframes(search_tweet, today)\n",
    "    \n",
    "else:\n",
    "    main_tweets, main_users = create_dataframes(search_tweet, today)\n",
    "    main_places = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 6)\n",
      "(99, 4)\n"
     ]
    }
   ],
   "source": [
    "print(main_tweets.shape)\n",
    "print(main_users.shape)\n",
    "#print(main_places.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Request more data\n",
    "The `search_tweets` function was build to only resquest tweets one time, nevertheless, with the `next_token` parameter we can easily request more data. This parameter indicates that there are more \"pages\" or more results (tweets) that matches the query it was previously sent to Twitter API. If the `next_token` parameter is found in the returned dictionary, then it means there are more results than the ones first returned. If this parameter is missing, then there are no more tweets regarding this topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 b26v89c19zqg8o3fpdy7o05sa8x65gw7tv2ku33a088e5\n",
      "2 b26v89c19zqg8o3fpdy7o056yh9o5asmxno5dx90pq7p9\n",
      "3 b26v89c19zqg8o3fpdy7o04w4kbzn2voh1ipfxpkv8k1p\n",
      "4 b26v89c19zqg8o3fpdy7o04l7l39cghor9pv2blplxmd9\n",
      "5 b26v89c19zqg8o3fpdy7o04ac4wgby8p5xwzzuovosfwd\n",
      "6 b26v89c19zqg8o3fpdy7o03zgn7q8ac6ob37au1s0gfi5\n",
      "7 b26v89c19zqg8o3fpdy7ny1nywaqealk664d3c0lrhxfh\n",
      "8 b26v89c19zqg8o3fpdy7ny1d4x8jhastbo69sxp3n9uv1\n",
      "9 b26v89c19zqg8o3fpdy7ny12ch7uoce6fqu2hlvkhu8al\n",
      "10 b26v89c19zqg8o3fpdy7ny0rikxhhlilv6kz24udczrel\n",
      "11 b26v89c19zqg8o3fpdy7ny0glk6yer1vmn79bxv65gwal\n",
      "12 b26v89c19zqg8o3fpdy7ny05t5o5mocb440yn84kplnct\n",
      "13 b26v89c19zqg8o3fpdy7nxzv27uksjviyyevc39r7v5h9\n",
      "14 b26v89c19zqg8o3fpdy7nxzk8ax74erjp2cwid0c3k871\n",
      "15 b26v89c19zqg8o3fpdy7nxz9bb1c0e8rs398obyweb9fh\n",
      "16 b26v89c19zqg8o3fpdy79bmju39w2k7mcag05tnozxw8t\n",
      "17 b26v89c19zqg8o3fpdy79bm935vm5sqjey59kxu81txtp\n",
      "18 b26v89c19zqg8o3fpdy79blydrr6njc017yrvc7epid8d\n",
      "19 b26v89c19zqg8o3fpdy79blnjv8t1rq7u48aaaykgau0t\n",
      "20 b26v89c19zqg8o3fpdy79blcmw7zxszxh3qasp3b74ol9\n",
      "21 b26v89c19zqg8o3fpdy79bl1vxyyrvt1zk7100cobqxrx\n",
      "22 b26v89c19zqg8o3fpdy79bkr522ktvdw5vx830cqsfqm5\n",
      "23 b26v89c19zqg8o3fpdy79bjv2dnxpkfie4az9bogwdh8d\n",
      "24 b26v89c19zqg8o3fpdy799hj3wep6nlhz3n9k2nrynbel\n",
      "25 b26v89c19zqg8o3fpdy799gxhj1g66b9wxni4nnb23ssd\n",
      "26 b26v89c19zqg8o3fpdy799gbzqdy60aj6r4g0pfzt7ipp\n",
      "27 b26v89c19zqg8o3fpdy799g117gdakejzapo4l441bzi5\n",
      "28 b26v89c19zqg8o3fpdy799ffnyghgmftl6nplkh1rkdj1\n",
      "29 b26v89c19zqg8o3fpdy799f4jeyf2lpvipk2qir8vwubh\n",
      "30 b26v89c19zqg8o3fpdy797csx2w2jvu6d6yqpkwu281vh\n"
     ]
    }
   ],
   "source": [
    "# for i in range(1, 16) # ~750\n",
    "# for i in range(1, 21): # ~1000\n",
    "for i in range(1, 31):\n",
    "    \n",
    "    # Check if there is a next token (another page)\n",
    "    # that matches the desired query\n",
    "    if 'next_token' in search_tweet['meta'].keys():\n",
    "        print(i, search_tweet[\"meta\"][\"next_token\"])\n",
    "\n",
    "        # Collect data from next token\n",
    "        new_tweets = search_tweets(query = \"Black Widow\", next_token = search_tweet['meta']['next_token'])\n",
    "        search_tweet = new_tweets\n",
    "\n",
    "        # Check if any tweet has enabled the location,\n",
    "        # so we can create the places dataframe.\n",
    "        if \"places\" in search_tweet['includes'].keys():\n",
    "            tweets, users, places = create_dataframes(search_tweet, today = today)\n",
    "\n",
    "            # Append data to main tweets\n",
    "            main_tweets = main_tweets.append(tweets)\n",
    "            main_users = main_users.append(users)\n",
    "            main_places = main_places.append(places)\n",
    "\n",
    "            # Reset index\n",
    "            main_tweets = main_tweets.reset_index(drop = True)\n",
    "            main_users = main_users.reset_index(drop = True)\n",
    "            main_places = main_places.reset_index(drop = True)\n",
    "\n",
    "        # If any tweet has its location enabled, then only\n",
    "        # create the other two dataframes.\n",
    "        else: \n",
    "            tweets, users = create_dataframes(search_tweet, today = today)\n",
    "\n",
    "            # Append data to main tweets\n",
    "            main_tweets = main_tweets.append(tweets)\n",
    "            main_users = main_users.append(users)\n",
    "\n",
    "            # Reset index\n",
    "            main_tweets = main_tweets.reset_index(drop = True)\n",
    "            main_users = main_users.reset_index(drop = True)\n",
    "\n",
    "    # If there are not more results regarding the\n",
    "    # requested topic, then just stop requesting \n",
    "    # more data.\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2350, 6)\n",
      "(2246, 4)\n",
      "(20, 4)\n"
     ]
    }
   ],
   "source": [
    "print(main_tweets.shape)\n",
    "print(main_users.shape)\n",
    "print(main_places.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xxxxxxxxxxxxxxxxxxxxxxxx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-bb7a8a5418d0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mxxxxxxxxxxxxxxxxxxxxxxxx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'xxxxxxxxxxxxxxxxxxxxxxxx' is not defined"
     ]
    }
   ],
   "source": [
    "xxxxxxxxxxxxxxxxxxxxxxxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the data locally\n",
    "if main_places.empty:\n",
    "    main_tweets.to_csv(f\"data/tweets/tweets_{today}_part3.csv\", index = False)\n",
    "    main_users.to_csv(f\"data/users/users_{today}_part3.csv\", index = False)\n",
    "    \n",
    "else:\n",
    "    main_tweets.to_csv(f\"data/tweets/tweets_{today}_part3.csv\", index = False)\n",
    "    main_users.to_csv(f\"data/users/users_{today}_part3.csv\", index = False)\n",
    "    main_places.to_csv(f\"data/places/places_{today}_part3.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Clean data\n",
    "Before implementing the algorithm, we should start by cleaning and pre-processing our data, in this case, the papers csv is already loaded. The pre-processing phase includes the following steps and it's performed with help of the `PreProcessor` class:\n",
    "\n",
    "- **Remove noise:** Noise removal is about removing characters digits and pieces of text that can interfere with text analysis. Noise removal is one of the most essential text preprocessing steps.\n",
    "\n",
    "\n",
    "- **Normalize text:** Text normalization is the process of transforming a text into a canonical (standard) form. For example, the word “gooood” and “gud” can be transformed to “good”, its canonical form. \n",
    "\n",
    "\n",
    "- **Tokenization:** Tokenization is a way of separating a piece of text into smaller units called tokens. In this case tokens are words (but can also be characters or subwords).\n",
    "\n",
    "\n",
    "- **Stemming:** Stemming is the process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words (known as a lemma).\n",
    "\n",
    "\n",
    "- **Lemmatization:** Lemmatization is a method responsible for grouping different inflected forms of words into the root form, having the same meaning. It is similar to stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessor:\n",
    "    \n",
    "    def __init__(self, regex_dict = None):\n",
    "        \n",
    "        # creating classes\n",
    "        # stem\n",
    "        self.sb = nltk.stem.SnowballStemmer('english')\n",
    "        \n",
    "        # lemmatize\n",
    "        self.lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "        \n",
    "        # translate\n",
    "        self.translator = Translator()\n",
    "        \n",
    "        # declare a default regex dict\n",
    "        self.default_regex_dict = {'goo[o]*d':'good', '2morrow':'tomorrow', 'b4':'before', 'otw':'on the way',\n",
    "                                   'idk':\"i don't know\", ':)':'smile', 'bc':'because', '2nite':'tonight',\n",
    "                                   'yeah':'yes', 'yeshhhhhhhh':'yes', ' yeeeee':'yes', 'btw':'by the way', \n",
    "                                   'fyi':'for your information', 'gr8':'great', 'asap':'as soon as possible', \n",
    "                                   'yummmmmy':'yummy', 'gf':'girlfriend', 'thx':'thanks','nowwwwwww':'now', \n",
    "                                   ' ppl ':' people ', 'yeiii':'yes'}\n",
    "        \n",
    "        # if no regex_dict defined by user, then use \n",
    "        # one by default. Else, concat two regex dicts\n",
    "        if regex_dict:            \n",
    "            self.regex_dict = {**regex_dict, **default_regex_dict}\n",
    "            \n",
    "        else:\n",
    "            self.regex_dict = self.default_regex_dict\n",
    "    \n",
    "    def translate_twt(self, pdf):\n",
    "    \n",
    "        \"\"\"\n",
    "        This function helps to translate a tweet from any \n",
    "        language to English.\n",
    "\n",
    "        Inputs:\n",
    "            - pdf: Pandas dataframe. This dataframe must have\n",
    "               the following columns:\n",
    "                - lang: Tweet's language.\n",
    "                - clean_tweet: Partially pre-processed tweet.\n",
    "\n",
    "        Outputs: Translated tweet from any language available \n",
    "                 in googletrans api to English.\n",
    "        \"\"\"\n",
    "\n",
    "        # Check if the language of the tweet is either undefined or English\n",
    "        # to avoid translation.\n",
    "        if pdf[\"lang\"] == \"und\" or pdf[\"lang\"] == \"en\":\n",
    "            pdf[\"translated_tweet\"] = pdf[\"clean_tweet\"]\n",
    "\n",
    "        # Check if tweet is in Hindi. The code of Hindi language is \"hi\", but \n",
    "        # Twitter has defined the code as \"in\".\n",
    "        elif pdf[\"lang\"] == \"in\":\n",
    "            pdf[\"translated_tweet\"] = self.translator.translate(pdf[\"clean_tweet\"], src = \"hi\", dest = \"en\").text\n",
    "            \n",
    "        # Check if tweet is in Chinese. \n",
    "        # The api supports simplified and traditional chinese.\n",
    "        elif pdf[\"lang\"] == \"zh\":\n",
    "            pdf[\"translated_tweet\"] = self.translator.translate(pdf[\"clean_tweet\"], src = \"zh-cn\", dest = \"en\").text\n",
    "\n",
    "        # For any other language the translator should work just fine, so the\n",
    "        # api should work with the language detected by Twitter.\n",
    "        else:\n",
    "            pdf[\"translated_tweet\"] = self.translator.translate(pdf[\"clean_tweet\"], src = pdf[\"lang\"], dest = \"en\").text\n",
    "\n",
    "        return pdf[\"translated_tweet\"]\n",
    "\n",
    "    \n",
    "    def removeNoise(self, pdf):\n",
    "        \n",
    "        \"\"\"\n",
    "        Function to remove noise from strings. \n",
    "        \n",
    "        Inputs: A pandas dataframe with raw strings of length n.\n",
    "        \n",
    "        Output: A clean string where elements such as accented \n",
    "        words, html tags, punctuation marks, and extra white \n",
    "        spaces will be removed (or transform) if it's the case.\n",
    "        \"\"\"\n",
    "        \n",
    "        # to lower case\n",
    "        pdf[\"clean_tweet\"] = pdf.text.apply(lambda x: x.lower())\n",
    "        \n",
    "        # remove accented characters from string\n",
    "        # e.g. canción --> cancion\n",
    "        pdf[\"clean_tweet\"] = pdf.clean_tweet.apply(lambda x: unidecode.unidecode(x))\n",
    "        \n",
    "        # remove html tags \n",
    "        pdf[\"clean_tweet\"] = pdf.clean_tweet.str.replace(r'<[^<>]*>', '', regex=True)\n",
    "        \n",
    "        # remove (match with) usernames | hashtags | punct marks | links\n",
    "        # punct marks = \",.':!?;\n",
    "        # do not remove: ' \n",
    "        # but remove: \"\n",
    "        pdf[\"clean_tweet\"] = pdf.clean_tweet.apply(lambda x:' '.join(re.sub(\"(@[A-Za-z0-9]+)|(#[A-Za-z0-9]+)|([-.,:_;])|(https?:\\/\\/.*[\\r\\n]*)\",\n",
    "                                                                            \" \", x).split()).replace('\"',''))\n",
    "                \n",
    "        # remove white spaces at the begining and at \n",
    "        # the end of a string\n",
    "        pdf['clean_tweet'] = pdf.clean_tweet.apply(lambda x: x.lstrip(' '))\n",
    "        pdf['clean_tweet'] = pdf.clean_tweet.apply(lambda x: x.rstrip(' '))\n",
    "        \n",
    "        # Translate tweet\n",
    "        pdf[\"clean_tweet\"] = pdf.apply(lambda x: self.translate_twt(x), axis = 1)\n",
    "        \n",
    "        # normalize string\n",
    "        # normalize accented charcaters and other strange characters\n",
    "        # NFKD if there are accented characters (????\n",
    "        pdf[\"clean_tweet\"] = pdf.clean_tweet.apply(lambda x: unicodedata.normalize('NFKC', x).encode('ASCII', 'ignore').decode(\"utf-8\"))\n",
    "        \n",
    "        return pdf\n",
    "    \n",
    "    \n",
    "    def textNormalization(self, pdf):\n",
    "        \n",
    "        \"\"\"\n",
    "        Function to normalize a string. \n",
    "        \n",
    "        Inputs: A pandas dataframe with strings (of length n) that \n",
    "        will be normalized. \n",
    "        \n",
    "        Outputs: A normalized string whitout noise, words in their\n",
    "        (expected) correct form and with no stopwords.\n",
    "        \"\"\"\n",
    "        \n",
    "        # remove noise first\n",
    "        pdf = self.removeNoise(pdf)\n",
    "\n",
    "        # expand contractions\n",
    "        # e.g. don't --> do not\n",
    "        pdf['clean_tweet'] = pdf.clean_tweet.apply(lambda x: contractions.fix(x))\n",
    " \n",
    "        # Normalize words\n",
    "        pdf['clean_tweet'] = pdf.clean_tweet.replace(self.regex_dict)\n",
    "                \n",
    "        # get English stopwords    \n",
    "        stop_words = stopwords.words('english')\n",
    "        stopwords_dict = Counter(stop_words)\n",
    "        \n",
    "        # remove stopwords from string\n",
    "        pdf[\"clean_tweet\"] = pdf.clean_tweet.apply(lambda x: ' '.join([word for word in x.split()\n",
    "                                                                       if word not in stopwords_dict]))\n",
    "            \n",
    "        return pdf\n",
    "    \n",
    "    \n",
    "    def wordTokenize(self, pdf):\n",
    "        \"\"\"\n",
    "        Function to tokenize a string into words. Tokenization is a way \n",
    "        of separating a piece of text into smaller units called tokens.\n",
    "        In this case tokens are words (but can also be characters or \n",
    "        subwords).\n",
    "        \n",
    "        Inputs: A pandas dataframe with strings (of length n) that will be tokenized. \n",
    "        \n",
    "        Outputs: A list of tokenized words.\n",
    "        \"\"\"\n",
    "        # string normalized\n",
    "        #normalized = self.textNormalization(string)\n",
    "        pdf = self.textNormalization(pdf)\n",
    "        \n",
    "        # Use word_tokenize method to split the string\n",
    "        # into individual words. By default it returns\n",
    "        # a list.\n",
    "        pdf[\"clean_tweet\"] = pdf.clean_tweet.apply(lambda x: nltk.word_tokenize(x))        \n",
    "        \n",
    "        # Using isalpha() will help us to only keep\n",
    "        # items from the alphabet (no punctuation\n",
    "        # marks). \n",
    "        #pdf[\"clean_tweet\"] = pdf.clean_tweet.apply(lambda x: [word for word in x if word.isalpha()])\n",
    "        \n",
    "        # Keep only unique elements\n",
    "        pdf[\"clean_tweet\"] = pdf.clean_tweet.apply(lambda x: list(set(x)))\n",
    "\n",
    "        # return list of tokenized words by row\n",
    "        return pdf\n",
    "    \n",
    "    def phraseTokenize(self, pdf):\n",
    "        \n",
    "        \"\"\"\n",
    "        Function to tokenize a string into sentences. Tokenization is\n",
    "        a way of separating a piece of text into smaller units called\n",
    "        tokens. In this case tokens are phrases (but can also be words,\n",
    "        characters or subwords).\n",
    "        \n",
    "        Inputs: A string (of length n) that will be tokenized. \n",
    "        \n",
    "        Outputs: A list of tokenized sentences.\n",
    "        \"\"\"\n",
    "        \n",
    "        # pandas dataframe with strings normalized\n",
    "        pdf = self.textNormalization(pdf)\n",
    "        \n",
    "        # Use sent_tokenize method to split the string\n",
    "        # into sentences. By default it returns a list.\n",
    "        pdf[\"clean_tweet\"] = pdf.clean_tweet.apply(lambda x: nltk.sent_tokenize(x))   \n",
    "        \n",
    "        return pdf \n",
    "    \n",
    "    \n",
    "    def stemWords(self, pdf):\n",
    "        \n",
    "        \"\"\"\n",
    "        Function to stem strings. Stemming is the process of reducing\n",
    "        a word to its word stem that affixes to suffixes and prefixes \n",
    "        or to the roots of words (known as a lemma).\n",
    "        \n",
    "        Inputs: A raw string of length n.\n",
    "        \n",
    "        Output: Roots of each word of a given string.\n",
    "        \"\"\"\n",
    "        \n",
    "        # pandas dataframe with strings normalized\n",
    "        pdf = self.textNormalization(pdf)\n",
    "        \n",
    "        # tokenized string (into words)\n",
    "        pdf = self.wordTokenize(data)\n",
    "            \n",
    "        # reduct words to its root    \n",
    "        pdf[\"clean_tweet\"] = pdf.clean_tweet.apply(lambda x: [self.sb.stem(word) for word in x])\n",
    "        \n",
    "        return pdf\n",
    "    \n",
    "    \n",
    "    def lemmatizeWords(self, pdf):\n",
    "        \n",
    "        \"\"\"\n",
    "        Function to lemmatize strings. Lemmatization is a method \n",
    "        responsible for grouping different inflected forms of \n",
    "        words into the root form, having the same meaning. It is \n",
    "        similar to stemming.\n",
    "        \n",
    "        Inputs: A raw string of length n.\n",
    "        \n",
    "        Output: Roots of each word of a given string (with better\n",
    "        performance than in stemming).\n",
    "        \"\"\"\n",
    "        \n",
    "        # pandas dataframe with strings normalized\n",
    "        pdf = self.textNormalization(pdf)\n",
    "        \n",
    "        # list of tokenized words (from string)\n",
    "        # Here it was decided to tokenize by words\n",
    "        # rather than by sentences due to we thought\n",
    "        # it would be easier to find the correct roots\n",
    "        # of each word.\n",
    "        pdf = self.wordTokenize(pdf)\n",
    "        \n",
    "        # lematize word from list of tokenized words\n",
    "        #lematized = [self.lemmatizer.lemmatize(word) for word in tokenized]\n",
    "        pdf[\"clean_tweet\"] = pdf.clean_tweet.apply(lambda x: [self.lemmatizer.lemmatize(word) for word in x])\n",
    "        \n",
    "        return pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2350, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>lang</th>\n",
       "      <th>possibly_sensitive</th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>clean_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1465177761631199238</td>\n",
       "      <td>tl</td>\n",
       "      <td>False</td>\n",
       "      <td>2021-11-29T04:36:14.000Z</td>\n",
       "      <td>grabe makahilak ang after credits sa black wid...</td>\n",
       "      <td>None</td>\n",
       "      <td>[black, widow, makahilak, credit, grabe, ang, sa]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1465177752881958919</td>\n",
       "      <td>th</td>\n",
       "      <td>False</td>\n",
       "      <td>2021-11-29T04:36:12.000Z</td>\n",
       "      <td>แพลนหลังสอบเสร็จ\\n-Black Widow\\n-ซีรีส์วันด้า\\...</td>\n",
       "      <td>None</td>\n",
       "      <td>[besrcch, widow, black, /, (, `, ncis, chiirii...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweet_id lang  possibly_sensitive                created_at  \\\n",
       "0  1465177761631199238   tl               False  2021-11-29T04:36:14.000Z   \n",
       "1  1465177752881958919   th               False  2021-11-29T04:36:12.000Z   \n",
       "\n",
       "                                                text  type  \\\n",
       "0  grabe makahilak ang after credits sa black wid...  None   \n",
       "1  แพลนหลังสอบเสร็จ\\n-Black Widow\\n-ซีรีส์วันด้า\\...  None   \n",
       "\n",
       "                                         clean_tweet  \n",
       "0  [black, widow, makahilak, credit, grabe, ang, sa]  \n",
       "1  [besrcch, widow, black, /, (, `, ncis, chiirii...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create class object\n",
    "pre_processor = PreProcessor()\n",
    "\n",
    "# Clean data and only keep \n",
    "# the roots of each word.\n",
    "#tweets['clean_tweet'] = tweets.text.apply(pre_processor.removeNoise)\n",
    "main_tweets = pre_processor.lemmatizeWords(main_tweets)\n",
    "print(main_tweets.shape)\n",
    "main_tweets.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the data locally\n",
    "main_tweets.to_csv(f'data/clean_tweets/clean_tweets_{today}_part3.csv', index = False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "char = '✅'\n",
    "char"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
