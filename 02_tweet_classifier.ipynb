{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import unidecode\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import contractions\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from googletrans import Translator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import models, layers, callbacks, preprocessing\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jigsaw_tweets = pd.read_csv(\"jigsaw_toxic_data/train.csv\")\n",
    "print(jigsaw_tweets.shape)\n",
    "jigsaw_tweets.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clean data\n",
    "Before implementing the algorithm, we should start by cleaning and pre-processing our data, in this case, the papers csv is already loaded. The pre-processing phase includes the following steps and it's performed with help of the `PreProcessor` class:\n",
    "\n",
    "- **Remove noise:** Noise removal is about removing characters digits and pieces of text that can interfere with text analysis. Noise removal is one of the most essential text preprocessing steps.\n",
    "\n",
    "\n",
    "- **Normalize text:** Text normalization is the process of transforming a text into a canonical (standard) form. For example, the word “gooood” and “gud” can be transformed to “good”, its canonical form. \n",
    "\n",
    "\n",
    "- **Tokenization:** Tokenization is a way of separating a piece of text into smaller units called tokens. In this case tokens are words (but can also be characters or subwords).\n",
    "\n",
    "\n",
    "- **Stemming:** Stemming is the process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words (known as a lemma).\n",
    "\n",
    "\n",
    "- **Lemmatization:** Lemmatization is a method responsible for grouping different inflected forms of words into the root form, having the same meaning. It is similar to stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessor:\n",
    "    \n",
    "    def __init__(self, regex_dict = None):\n",
    "        \n",
    "        # creating classes\n",
    "        # stem\n",
    "        self.sb = nltk.stem.SnowballStemmer('english')\n",
    "        \n",
    "        # lemmatize\n",
    "        self.lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "        \n",
    "        # translate\n",
    "        self.translator = Translator()\n",
    "        \n",
    "        # declare a default regex dict\n",
    "        self.default_regex_dict = {'goo[o]*d':'good', '2morrow':'tomorrow', 'b4':'before', 'otw':'on the way',\n",
    "                                   'idk':\"i don't know\", ':)':'smile', 'bc':'because', '2nite':'tonight',\n",
    "                                   'yeah':'yes', 'yeshhhhhhhh':'yes', ' yeeeee':'yes', 'btw':'by the way', \n",
    "                                   'fyi':'for your information', 'gr8':'great', 'asap':'as soon as possible', \n",
    "                                   'yummmmmy':'yummy', 'gf':'girlfriend', 'thx':'thanks','nowwwwwww':'now', \n",
    "                                   ' ppl ':' people ', 'yeiii':'yes'}\n",
    "        \n",
    "        # if no regex_dict defined by user, then use \n",
    "        # one by default. Else, concat two regex dicts\n",
    "        if regex_dict:            \n",
    "            self.regex_dict = {**regex_dict, **default_regex_dict}\n",
    "            \n",
    "        else:\n",
    "            self.regex_dict = self.default_regex_dict\n",
    "    \n",
    "    def removeNoise(self, pdf):\n",
    "        \n",
    "        \"\"\"\n",
    "        Function to remove noise from strings. \n",
    "        \n",
    "        Inputs: A pandas dataframe with raw strings of length n.\n",
    "        \n",
    "        Output: A clean string where elements such as accented \n",
    "        words, html tags, punctuation marks, and extra white \n",
    "        spaces will be removed (or transform) if it's the case.\n",
    "        \"\"\"\n",
    "        \n",
    "        # to lower case\n",
    "        pdf[\"clean_tweet\"] = pdf.comment_text.apply(lambda x: x.lower())\n",
    "        \n",
    "        # remove accented characters from string\n",
    "        # e.g. canción --> cancion\n",
    "        pdf[\"clean_tweet\"] = pdf.clean_tweet.apply(lambda x: unidecode.unidecode(x))\n",
    "        \n",
    "        # remove html tags \n",
    "        pdf[\"clean_tweet\"] = pdf.clean_tweet.str.replace(r'<[^<>]*>', '', regex=True)\n",
    "        \n",
    "        # remove (match with) usernames | hashtags | punct marks | links\n",
    "        # punct marks = \",.':!?;\n",
    "        # do not remove: ' \n",
    "        # but remove: \"\n",
    "        pdf[\"clean_tweet\"] = pdf.clean_tweet.apply(lambda x:' '.join(re.sub(\"(@[A-Za-z0-9]+)|(#[A-Za-z0-9]+)|([-.,:_;])|(https?:\\/\\/.*[\\r\\n]*)\",\n",
    "                                                                            \" \", x).split()).replace('\"',''))\n",
    "                \n",
    "        # remove white spaces at the begining and at \n",
    "        # the end of a string\n",
    "        pdf['clean_tweet'] = pdf.clean_tweet.apply(lambda x: x.lstrip(' '))\n",
    "        pdf['clean_tweet'] = pdf.clean_tweet.apply(lambda x: x.rstrip(' '))\n",
    "        \n",
    "        # normalize string\n",
    "        # normalize accented charcaters and other strange characters\n",
    "        # NFKD if there are accented characters (????\n",
    "        pdf[\"clean_tweet\"] = pdf.clean_tweet.apply(lambda x: unicodedata.normalize('NFKC', x).encode('ASCII', 'ignore').decode(\"utf-8\"))\n",
    "        \n",
    "        return pdf\n",
    "    \n",
    "    \n",
    "    def textNormalization(self, pdf):\n",
    "        \n",
    "        \"\"\"\n",
    "        Function to normalize a string. \n",
    "        \n",
    "        Inputs: A pandas dataframe with strings (of length n) that \n",
    "        will be normalized. \n",
    "        \n",
    "        Outputs: A normalized string whitout noise, words in their\n",
    "        (expected) correct form and with no stopwords.\n",
    "        \"\"\"\n",
    "        \n",
    "        # remove noise first\n",
    "        pdf = self.removeNoise(pdf)\n",
    "\n",
    "        # expand contractions\n",
    "        # e.g. don't --> do not\n",
    "        pdf['clean_tweet'] = pdf.clean_tweet.apply(lambda x: contractions.fix(x))\n",
    "         \n",
    "        # Normalize words\n",
    "        pdf['clean_tweet'] = pdf.clean_tweet.replace(self.regex_dict)\n",
    "                \n",
    "        # get English stopwords    \n",
    "        stop_words = stopwords.words('english')\n",
    "        stopwords_dict = Counter(stop_words)\n",
    "        \n",
    "        # remove stopwords from string\n",
    "        pdf[\"clean_tweet\"] = pdf.clean_tweet.apply(lambda x: ' '.join([word for word in x.split()\n",
    "                                                                       if word not in stopwords_dict]))\n",
    "        return pdf\n",
    "    \n",
    "    def wordTokenize(self, pdf):\n",
    "        \"\"\"\n",
    "        Function to tokenize a string into words. Tokenization is a way \n",
    "        of separating a piece of text into smaller units called tokens.\n",
    "        In this case tokens are words (but can also be characters or \n",
    "        subwords).\n",
    "        \n",
    "        Inputs: A pandas dataframe with strings (of length n) that will be tokenized. \n",
    "        \n",
    "        Outputs: A list of tokenized words.\n",
    "        \"\"\"\n",
    "        # string normalized\n",
    "        #normalized = self.textNormalization(string)\n",
    "        pdf = self.textNormalization(pdf)\n",
    "        \n",
    "        # Use word_tokenize method to split the string\n",
    "        # into individual words. By default it returns\n",
    "        # a list.\n",
    "        pdf[\"clean_tweet\"] = pdf.clean_tweet.apply(lambda x: nltk.word_tokenize(x))        \n",
    "        \n",
    "        # Using isalpha() will help us to only keep\n",
    "        # items from the alphabet (no punctuation\n",
    "        # marks). \n",
    "        pdf[\"clean_tweet\"] = pdf.clean_tweet.apply(lambda x: [word for word in x if word.isalpha()])\n",
    "        \n",
    "        # Keep only unique elements\n",
    "        pdf[\"clean_tweet\"] = pdf.clean_tweet.apply(lambda x: list(set(x)))\n",
    "\n",
    "        # return list of tokenized words by row\n",
    "        return pdf\n",
    "    \n",
    "    \n",
    "    def phraseTokenize(self, pdf):\n",
    "        \n",
    "        \"\"\"\n",
    "        Function to tokenize a string into sentences. Tokenization is\n",
    "        a way of separating a piece of text into smaller units called\n",
    "        tokens. In this case tokens are phrases (but can also be words,\n",
    "        characters or subwords).\n",
    "        \n",
    "        Inputs: A string (of length n) that will be tokenized. \n",
    "        \n",
    "        Outputs: A list of tokenized sentences.\n",
    "        \"\"\"\n",
    "        \n",
    "        # pandas dataframe with strings normalized\n",
    "        pdf = self.textNormalization(pdf)\n",
    "        \n",
    "        # Use sent_tokenize method to split the string\n",
    "        # into sentences. By default it returns a list.\n",
    "        pdf[\"clean_tweet\"] = pdf.clean_tweet.apply(lambda x: nltk.sent_tokenize(x))   \n",
    "        \n",
    "        return pdf \n",
    "    \n",
    "    \n",
    "    def stemWords(self, pdf):\n",
    "        \n",
    "        \"\"\"\n",
    "        Function to stem strings. Stemming is the process of reducing\n",
    "        a word to its word stem that affixes to suffixes and prefixes \n",
    "        or to the roots of words (known as a lemma).\n",
    "        \n",
    "        Inputs: A raw string of length n.\n",
    "        \n",
    "        Output: Roots of each word of a given string.\n",
    "        \"\"\"\n",
    "        \n",
    "        # pandas dataframe with strings normalized\n",
    "        pdf = self.textNormalization(pdf)\n",
    "        \n",
    "        # tokenized string (into words)\n",
    "        pdf = self.wordTokenize(data)\n",
    "            \n",
    "        # reduct words to its root    \n",
    "        pdf[\"clean_tweet\"] = pdf.clean_tweet.apply(lambda x: [self.sb.stem(word) for word in x])\n",
    "        \n",
    "        return pdf\n",
    "    \n",
    "    \n",
    "    def lemmatizeWords(self, pdf):\n",
    "        \n",
    "        \"\"\"\n",
    "        Function to lemmatize strings. Lemmatization is a method \n",
    "        responsible for grouping different inflected forms of \n",
    "        words into the root form, having the same meaning. It is \n",
    "        similar to stemming.\n",
    "        \n",
    "        Inputs: A raw string of length n.\n",
    "        \n",
    "        Output: Roots of each word of a given string (with better\n",
    "        performance than in stemming).\n",
    "        \"\"\"\n",
    "        \n",
    "        # pandas dataframe with strings normalized\n",
    "        pdf = self.textNormalization(pdf)\n",
    "        \n",
    "        # list of tokenized words (from string)\n",
    "        # Here it was decided to tokenize by words\n",
    "        # rather than by sentences due to we thought\n",
    "        # it would be easier to find the correct roots\n",
    "        # of each word.\n",
    "        pdf = self.wordTokenize(pdf)\n",
    "        \n",
    "        # lematize word from list of tokenized words\n",
    "        #lematized = [self.lemmatizer.lemmatize(word) for word in tokenized]\n",
    "        pdf[\"clean_tweet\"] = pdf.clean_tweet.apply(lambda x: [self.lemmatizer.lemmatize(word) for word in x])\n",
    "        \n",
    "        return pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>clean_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[please, since, edits, username, york, metalli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[I, utc, stuck, january, background, talk, mat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \\\n",
       "0             0        0       0       0              0   \n",
       "1             0        0       0       0              0   \n",
       "\n",
       "                                         clean_tweet  \n",
       "0  [please, since, edits, username, york, metalli...  \n",
       "1  [I, utc, stuck, january, background, talk, mat...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create class object\n",
    "pre_processor = PreProcessor()\n",
    "\n",
    "# Clean data and only keep the roots of each word\n",
    "jigsaw_tweets = pre_processor.lemmatizeWords(jigsaw_tweets)\n",
    "jigsaw_tweets.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model\n",
    "### 3.1 Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
