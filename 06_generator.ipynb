{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2075, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>possibly_sensitive</th>\n",
       "      <th>type</th>\n",
       "      <th>clean_tweet</th>\n",
       "      <th>toxic</th>\n",
       "      <th>pred_scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-11-21 20:17:03+00:00</td>\n",
       "      <td>An Israeli doctor says he believes he caught t...</td>\n",
       "      <td>en</td>\n",
       "      <td>1466139299263533060</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['people', ' say', ' belief', ' doctor', ' lon...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.895560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2021-11-21 20:17:03+00:00</td>\n",
       "      <td>meu deus eu preciso fazer processos de emenda ...</td>\n",
       "      <td>pt</td>\n",
       "      <td>1466139299137695747</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['de', ' emenda', ' meu', ' pra', ' e', ' faze...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998193</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  created_at  \\\n",
       "3  2021-11-21 20:17:03+00:00   \n",
       "6  2021-11-21 20:17:03+00:00   \n",
       "\n",
       "                                                text lang  \\\n",
       "3  An Israeli doctor says he believes he caught t...   en   \n",
       "6  meu deus eu preciso fazer processos de emenda ...   pt   \n",
       "\n",
       "              tweet_id  possibly_sensitive type  \\\n",
       "3  1466139299263533060               False  NaN   \n",
       "6  1466139299137695747               False  NaN   \n",
       "\n",
       "                                         clean_tweet  toxic  pred_scores  \n",
       "3  ['people', ' say', ' belief', ' doctor', ' lon...      1     0.895560  \n",
       "6  ['de', ' emenda', ' meu', ' pra', ' e', ' faze...      1     0.998193  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('trained_model/preds/tweets_preds.csv')\n",
    "df = df[df[\"toxic\"] == 1] # keep positive comments\n",
    "print(df.shape)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ' '.join(df.text.values)\n",
    "text = text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mapping of unique chars to integers\n",
    "chars = sorted(list(set(text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  303934\n",
      "Total Vocab:  622\n"
     ]
    }
   ],
   "source": [
    "n_chars = len(text)\n",
    "n_vocab = len(chars)\n",
    "print( \"Total Characters: \", n_chars)\n",
    "print( \"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset of input to output pairs encoded as integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  303834\n"
     ]
    }
   ],
   "source": [
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = text[i:i + seq_length]\n",
    "    seq_out = text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print( \"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshape X to be [samples, time steps, features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the LSTM model - Choose among single and deep, comment one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single Model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the checkpoint\n",
    "Because of the slowness and because of our optimization requirements, we will use model checkpointing to record all of the network weights to file each time an improvement in loss is observed at the end of the epoch. We will use the best set of weights (lowest loss) to instantiate our generative model in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"trained_model/weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit model to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2374/2374 [==============================] - ETA: 0s - loss: 3.3028\n",
      "Epoch 00001: loss improved from inf to 3.30275, saving model to trained_model\\weights-improvement-01-3.3028.hdf5\n",
      "2374/2374 [==============================] - 965s 406ms/step - loss: 3.3028\n",
      "Epoch 2/5\n",
      "2374/2374 [==============================] - ETA: 0s - loss: 3.2775\n",
      "Epoch 00002: loss improved from 3.30275 to 3.27748, saving model to trained_model\\weights-improvement-02-3.2775.hdf5\n",
      "2374/2374 [==============================] - 991s 418ms/step - loss: 3.2775\n",
      "Epoch 3/5\n",
      "2374/2374 [==============================] - ETA: 0s - loss: 3.1850\n",
      "Epoch 00003: loss improved from 3.27748 to 3.18497, saving model to trained_model\\weights-improvement-03-3.1850.hdf5\n",
      "2374/2374 [==============================] - 1044s 440ms/step - loss: 3.1850\n",
      "Epoch 4/5\n",
      "2374/2374 [==============================] - ETA: 0s - loss: 3.0389\n",
      "Epoch 00004: loss improved from 3.18497 to 3.03886, saving model to trained_model\\weights-improvement-04-3.0389.hdf5\n",
      "2374/2374 [==============================] - 1022s 431ms/step - loss: 3.0389\n",
      "Epoch 5/5\n",
      "2374/2374 [==============================] - ETA: 0s - loss: 2.9661\n",
      "Epoch 00005: loss improved from 3.03886 to 2.96611, saving model to trained_model\\weights-improvement-05-2.9661.hdf5\n",
      "2374/2374 [==============================] - 976s 411ms/step - loss: 2.9661\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2a90738f130>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lesser epochs lesser time worst results\n",
    "model.fit(X, y, epochs=5, batch_size=128, callbacks = callbacks_list)\n",
    "\n",
    "# More epochs, more time, best results\n",
    "#model.fit(X, y, epochs=50, batch_size=64, callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the example, you should have a number of weight checkpoint files in the local directory.\n",
    "\n",
    "You can delete them all except the one with the smallest loss value. For example, when I ran this example, below was the checkpoint with the smallest loss that I achieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best checkpoint name here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating text\n",
    "\n",
    "Generating text using the trained LSTM network is relatively straightforward.\n",
    "\n",
    "Firstly, we load the data and define the network in exactly the same way, except the network weights are loaded from a checkpoint file and the network does not need to be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the network weights\n",
    "filename = \"trained_model/weights-improvement-05-2.9661.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse mapping\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions\n",
    "\n",
    "The simplest way to use the Keras LSTM model to make predictions is to first start off with a seed sequence as input, generate the next character then update the seed sequence to add the generated character on the end and trim off the first character. This process is repeated for as long as we want to predict new characters (e.g. a sequence of 1,000 characters in length).\n",
    "\n",
    "We can pick a random input pattern as our seed sequence, then print generated characters as we generate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" is it a myth? i have this comfort blanket that rhinovirus stops covid 😭😂 is it nuts !? gets me throu \"\n",
      "  oo poe  oo po  ho po  ho po  ho po  ho po  ho po  ho po  ho po  ho  oo to  ho  oo to  ho  oo to  ho  oo to  ho  oo to  ho  oo to  ho  oo to  ho  oo to  ho  oo to  ho  oo to  ho  oo to  ho  oo to  ho  oo to  ho  oo to  ho  oo to  ho  oo to  ho  oo to  ho  oo to  ho  oo to  ho  oo to  ho  oo to  ho  oo to  ho  oo to  ho  oo to  ho  oo to  ho  oo to  ho  oo to  ho  oo to  ho  oo to  ho  oo to  ho  oo to  ho  oo to  ho  oo to  ho  oo to  ho  oo to  ho  oo to  ho  oo to  ho  oo to  ho  oo to  ho  oo to  ho  oo to  ho  oo to  ho  oo to  ho  oo to  ho  oo to  ho  oo to  ho  oo to  ho  oo to  ho  oo to  ho  oo to  ho  oo to  ho  oo to  ho  oo to  ho  oo to  ho  oo to  ho  oo to  ho  oo to  ho  oo to  ho  oo to  ho  oo to  "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# pick a random seed\n",
    "start = np.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print( \"Seed:\")\n",
    "print( \"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "\tx = np.reshape(pattern, (1, len(pattern), 1))\n",
    "\tx = x / float(n_vocab)\n",
    "\tprediction = model.predict(x, verbose=0)\n",
    "\tindex = np.argmax(prediction)\n",
    "\tresult = int_to_char[index]\n",
    "\tseq_in = [int_to_char[value] for value in pattern]\n",
    "\tsys.stdout.write(result)\n",
    "\tpattern.append(index)\n",
    "\tpattern = pattern[1:len(pattern)]\n",
    "print( \"\\nDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10 Extension Ideas to Improve the Model\n",
    "\n",
    "\n",
    "Predict fewer than 1,000 characters as output for a given seed.\n",
    "\n",
    "Remove all punctuation from the source text, and therefore from the models’ vocabulary.\n",
    "\n",
    "Try a one hot encoded for the input sequences.\n",
    "\n",
    "Train the model on padded sentences rather than random sequences of characters.\n",
    "\n",
    "Increase the number of training epochs to 100 or many hundreds.\n",
    "\n",
    "Add dropout to the visible input layer and consider tuning the dropout percentage.\n",
    "\n",
    "Tune the batch size, try a batch size of 1 as a (very slow) baseline and larger sizes from there.\n",
    "\n",
    "Add more memory units to the layers and/or more layers.\n",
    "\n",
    "Experiment with scale factors (temperature) when interpreting the prediction probabilities.\n",
    "\n",
    "Change the LSTM layers to be “stateful” to maintain state across batches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "interpreter": {
   "hash": "76e3749c4d8f143b85e17554ce3ae58a0fcf4212a15be7e0134aaba6048266e3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
